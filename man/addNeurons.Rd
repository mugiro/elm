% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/AllGenerics.R, R/SLFNclass.R
\docType{methods}
\name{addNeurons}
\alias{addNeurons}
\alias{addNeurons,SLFN-method}
\title{Add hidden neurons to the SLFN}
\usage{
addNeurons(object, ...)

\S4method{addNeurons}{SLFN}(object, type, number, W = NULL, B = NULL)
}
\arguments{
\item{object}{SLFN type model}

\item{...}{}

\item{type}{activation function of the added neurons ("linear", "sigmoid", "tan", "rbf").}

\item{number}{the number of hidden neurons added to the network}

\item{W}{input weight matrix of dimension [dxL]. List of centroids for rbf activation functions.}

\item{B}{input bias vector of dimension [1xL]. Vector of sigmas for rbf activation functions.}
}
\value{
object SLFN with new neurons added and W and B matrices updated.

It is called by the training wrapper when a new SLFN object is created. It is called sequentially
based on the different type of activation functions.

When addNeurons is called explicitly, the SLFN should be re-trained

For linear activation functions, the number of neurons added cannot be superior to the number of features (L=<d).
This case entails a linear projection of datato a higher dimensional, which yields a multicorrelated new space.
}
\description{
addNeurons adds a specific number of hidden neurons to the SLFN being all of them of the same type of activation function.
}
\section{Methods (by class)}{
\itemize{
\item \code{SLFN}: 
}}

